# ResearcherRAG ì‹¤ì „ ê°€ì´ë“œ: ì—°êµ¬ìë¥¼ ìœ„í•œ PRISMA + RAG ì›Œí¬í”Œë¡œìš°

## ğŸ¯ ë¬¸ì„œ ëª©ì 

ì´ ë¬¸ì„œëŠ” **ì‹¤ì œ ì—°êµ¬ìê°€ ìì‹ ì˜ ì—°êµ¬ í”„ë¡œì íŠ¸ì— ResearcherRAGë¥¼ ì ìš©**í•˜ëŠ” êµ¬ì²´ì  ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

AI failure_HR í”„ë¡œì íŠ¸ì˜ **ì‹¤ì œ PRISMA ì›Œí¬í”Œë¡œìš°**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬íšŒê³¼í•™Â·ì˜í•™Â·ê²½ì œí•™ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ ì—°êµ¬ìê°€ **ë¬¸í—Œ ìˆ˜ì§‘ â†’ í’ˆì§ˆ í•„í„°ë§ â†’ ë²¡í„°í™” â†’ RAG ì§ˆì˜**ê¹Œì§€ ì „ ê³¼ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ì•ˆë‚´í•©ë‹ˆë‹¤.

---

## ğŸ“š ì‚¬ë¡€ ì—°êµ¬: AI failure_HR í”„ë¡œì íŠ¸

### ì—°êµ¬ ê°œìš”

**ì—°êµ¬ ì œëª©**: "Mapping the Landscape of AI Risk in Human Resource Development"

**ì—°êµ¬ ì§ˆë¬¸**:
- RQ1: HR/HRD AI ë¦¬ìŠ¤í¬ ì—°êµ¬ëŠ” ì–´ë–¤ ì£¼ì œë¡œ êµ¬ì„±ë˜ëŠ”ê°€?
- RQ2: í† í”½ì´ ì‹œê°„ì— ë”°ë¼ ì–´ë–»ê²Œ ì§„í™”í–ˆëŠ”ê°€?
- RQ3: HRM vs HRD ì—°êµ¬ ì„±ìˆ™ë„ëŠ” ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€?

**PRISMA ì„±ê³¼**:
- ğŸ“Š **20,555í¸ ìˆ˜ì§‘** â†’ **592í¸ í¬í•¨** (2.9% ì„ íƒë¥ )
- ğŸ¯ **50% í•„í„°ë§ íš¨ê³¼** (ì¤‘ë³µ + ê´€ë ¨ì„±)
- âš¡ **ë‹¤ì°¨ì› í’ˆì§ˆ ì ìˆ˜** (0-50ì  ë²”ìœ„)

### PRISMA í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     IDENTIFICATION (20,555 papers)     â”‚
â”‚  OpenAlex: 11,772 | S2: 8,157         â”‚
â”‚  arXiv: 600 | CORE: 26                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     SCREENING (9,192 papers)           â”‚
â”‚  Dedup: -885 (DOI + Title)            â”‚
â”‚  No abstract: -10,478                  â”‚
â”‚  PRISMA score < 15: -8,600             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     INCLUDED (592 papers)              â”‚
â”‚  HRM: 471 (79.6%)                     â”‚
â”‚  HRD: 105 (17.7%)                     â”‚
â”‚  With PDFs: 316 (53.4%)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ ì—°êµ¬ìë¥¼ ìœ„í•œ 5ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°

### Phase 1: ì—°êµ¬ ì„¤ê³„ & ë°ì´í„° ìˆ˜ì§‘ (1-2ì£¼)

#### Step 1.1: ì—°êµ¬ ì§ˆë¬¸ ì •ì˜

**í•µì‹¬ ì›ì¹™**: ëª…í™•í•˜ê³  ì¸¡ì • ê°€ëŠ¥í•œ ì§ˆë¬¸

**ì˜ˆì‹œ (í•™ë¬¸ ë¶„ì•¼ë³„)**:

**ì‚¬íšŒê³¼í•™ (êµìœ¡í•™)**
```
RQ: "êµìœ¡ í˜„ì¥ì—ì„œ AI ë„ì… ì¥ë²½ì€ ë¬´ì—‡ì¸ê°€?"
- Domain: êµìœ¡ (education, teacher, student, school)
- Method: AI ê¸°ìˆ  (AI, digital tools, EdTech)
- Topic: ì±„íƒ ì¥ë²½ (barrier, challenge, resistance, acceptance)
```

**ì˜í•™**
```
RQ: "ì „ìê±´ê°•ê¸°ë¡(EHR) ë„ì…ì´ ì˜ì‚¬ ì—…ë¬´ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?"
- Domain: ì˜ë£Œ (physician, clinician, healthcare, hospital)
- Method: EHR ì‹œìŠ¤í…œ (EHR, EMR, electronic health record)
- Topic: ì—…ë¬´ ì˜í–¥ (workflow, burden, efficiency, satisfaction)
```

**ê²½ì œí•™**
```
RQ: "ìµœì €ì„ê¸ˆ ì¸ìƒì´ ê³ ìš©ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼ëŠ”?"
- Domain: ë…¸ë™ ì‹œì¥ (labor market, employment, unemployment)
- Method: ìµœì €ì„ê¸ˆ ì •ì±… (minimum wage, wage floor)
- Topic: ê³ ìš© íš¨ê³¼ (job loss, employment effect, elasticity)
```

**ì‹¬ë¦¬í•™**
```
RQ: "ì›ê²© ì‹¬ë¦¬ì¹˜ë£Œì˜ íš¨ê³¼ì„±ì€ ëŒ€ë©´ ì¹˜ë£Œì™€ ì°¨ì´ê°€ ìˆëŠ”ê°€?"
- Domain: ì‹¬ë¦¬ì¹˜ë£Œ (psychotherapy, counseling, mental health)
- Method: ì›ê²© ì¹˜ë£Œ (teletherapy, online therapy, telehealth)
- Topic: íš¨ê³¼ì„± (effectiveness, outcome, efficacy)
```

#### Step 1.2: Research Profile ìƒì„±

**íŒŒì¼ ê²½ë¡œ**: `config/research_profiles/my_research.yaml`

**í…œí”Œë¦¿**:
```yaml
name: "My Research Project Name"
description: "Brief description of your research focus"

domain_keywords:
  # ì—°êµ¬ ë„ë©”ì¸ (10-20ê°œ)
  - "your domain keyword 1"
  - "your domain keyword 2"
  # ì˜ˆ: education, teacher, student, classroom

method_keywords:
  # ë°©ë²•ë¡ /ê¸°ìˆ  (5-15ê°œ)
  - "your method keyword 1"
  - "your method keyword 2"
  # ì˜ˆ: AI, machine learning, digital tool

topic_keywords:
  # ì—°êµ¬ ì£¼ì œ/í˜„ìƒ (10-20ê°œ)
  - "your topic keyword 1"
  - "your topic keyword 2"
  # ì˜ˆ: adoption, barrier, challenge, acceptance

exclusion_keywords:
  # ì œì™¸í•  ë§¥ë½ (5-10ê°œ)
  - "irrelevant context 1"
  - "irrelevant context 2"
  # ì˜ˆ: medical, clinical (êµìœ¡ ì—°êµ¬ ì‹œ)

context_validators:
  # ë§¥ë½ ê²€ì¦ í‚¤ì›Œë“œ (5-10ê°œ)
  - "context keyword 1"
  - "context keyword 2"
  # ì˜ˆ: teacher, student, school (êµìœ¡ ë§¥ë½)

thresholds:
  screening: 25       # Stage 2 í†µê³¼ ìµœì†Œ ì ìˆ˜
  eligibility: 45     # Stage 3 í†µê³¼ ìµœì†Œ ì ìˆ˜
  review: 35          # ìˆ˜ë™ ê²€í†  í ìµœì†Œ ì ìˆ˜
  min_inclusion: 50   # Stage 4 ìµœì¢… í¬í•¨ ì ìˆ˜
```

**ì‹¤ì „ ì˜ˆì‹œ (êµìœ¡ AI ì±„íƒ ì—°êµ¬)**:
```yaml
name: "AI Adoption in K-12 Education"
description: "Barriers and facilitators of AI adoption in K-12 classroom"

domain_keywords:
  - "education"
  - "K-12"
  - "teacher"
  - "student"
  - "classroom"
  - "school"
  - "elementary"
  - "secondary"
  - "pedagogy"
  - "instruction"

method_keywords:
  - "artificial intelligence"
  - "AI"
  - "machine learning"
  - "digital tool"
  - "educational technology"
  - "EdTech"
  - "learning management system"
  - "intelligent tutoring"

topic_keywords:
  - "adoption"
  - "acceptance"
  - "barrier"
  - "challenge"
  - "resistance"
  - "facilitator"
  - "readiness"
  - "self-efficacy"
  - "attitude"
  - "perceived usefulness"

exclusion_keywords:
  - "medical education"
  - "clinical training"
  - "nursing education"
  - "anatomical education"
  - "surgery training"

context_validators:
  - "teacher"
  - "student"
  - "classroom"
  - "school"
  - "curriculum"
  - "lesson"

thresholds:
  screening: 30
  eligibility: 50
  review: 40
  min_inclusion: 55
```

#### Step 1.3: ë‹¤ì¤‘ ë°ì´í„°ë² ì´ìŠ¤ ìˆ˜ì§‘

**AI failure_HR ì‚¬ë¡€ (4ê°œ DB ì‚¬ìš©)**:
- OpenAlex: 11,772í¸
- Semantic Scholar: 8,157í¸
- arXiv: 600í¸
- CORE: 26í¸

**ì¶”ì²œ ì „ëµ (ì—°êµ¬ ë¶„ì•¼ë³„)**:

| ì—°êµ¬ ë¶„ì•¼ | Primary DB | Secondary DB | ì˜ˆìƒ ê·œëª¨ |
|----------|-----------|-------------|----------|
| **ì‚¬íšŒê³¼í•™** | OpenAlex | Semantic Scholar | 500-2,000 |
| **ì˜í•™** | PubMed/PMC | OpenAlex | 1,000-5,000 |
| **ê²½ì œí•™** | EconPapers (RePEc) | OpenAlex | 300-1,500 |
| **ì‹¬ë¦¬í•™** | PsycINFO | OpenAlex | 500-2,000 |
| **ì»´í“¨í„°ê³¼í•™** | arXiv | Semantic Scholar | 1,000-3,000 |

**ê²€ìƒ‰ ì¿¼ë¦¬ ì „ëµ**:

```python
# êµìœ¡ AI ì±„íƒ ì—°êµ¬ ì˜ˆì‹œ
queries = [
    # Core query (broad)
    '(artificial intelligence OR AI OR machine learning) AND (education OR teacher OR classroom) AND (adoption OR acceptance OR barrier)',

    # Specific applications
    '"AI adoption" AND education',
    '"educational technology acceptance"',
    '"teacher readiness" AND AI',

    # Theoretical frameworks
    '"technology acceptance model" AND education',
    '"TAM" OR "UTAUT" AND "AI" AND education',

    # Barriers focus
    '(barrier OR challenge OR resistance) AND AI AND teacher',

    # Self-efficacy
    '"self-efficacy" AND technology AND teacher'
]
```

#### Step 1.4: ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰

**ì½”ë“œ í…œí”Œë¦¿**:
```python
# collect_papers.py
from pyalex import Works
import pandas as pd
from datetime import datetime

# Research profile ë¡œë“œ
profile = ResearchProfile.from_yaml("config/research_profiles/my_research.yaml")

# OpenAlex ìˆ˜ì§‘
results = []
for query in queries:
    papers = Works().filter(
        default=query,
        publication_year='2015-2024',
        has_abstract=True,
        type='article|review',
        cited_by_count='>= 2'  # í’ˆì§ˆ í•„í„°
    ).get()

    results.extend(papers)

# CSV ì €ì¥
df = pd.DataFrame(results)
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
df.to_csv(f"data/raw/openalex_{profile.name}_{timestamp}.csv", index=False)
```

**AI failure_HR ì„±ê³¼**:
- ìˆ˜ì§‘ ì‹œê°„: ~2ì‹œê°„ (4ê°œ DB, 20,555í¸)
- ì €ì¥ ê²½ë¡œ: `data/raw/`
- íŒŒì¼ í˜•ì‹: CSV (title, abstract, doi, year, authors, url)

---

### Phase 2: PRISMA ìŠ¤í¬ë¦¬ë‹ (1-2ì¼)

#### Step 2.1: Stage 1 - Identification (ì¤‘ë³µ ì œê±°)

**AI failure_HR ì‚¬ë¡€**:
- ì…ë ¥: 20,555í¸
- ì¤‘ë³µ DOI ì œê±°: 93í¸
- ì¤‘ë³µ Title ì œê±°: 792í¸
- ì´ˆë¡ ì—†ìŒ ì œê±°: 10,478í¸
- ì¶œë ¥: 9,192í¸

**ì½”ë“œ (ìë™í™”)**:
```python
# deduplicate.py
import pandas as pd

df = pd.read_csv("data/raw/merged_all_sources.csv")

# 1. DOI ì¤‘ë³µ ì œê±°
print(f"Before dedup: {len(df)}")
initial = len(df)
df = df.drop_duplicates(subset=['doi'], keep='first')
print(f"After DOI dedup: {len(df)} (-{initial - len(df)})")

# 2. Title ì •ê·œí™” í›„ ì¤‘ë³µ ì œê±°
df['title_normalized'] = df['title'].str.lower().str.replace(r'[^\w\s]', '', regex=True)
df = df.drop_duplicates(subset=['title_normalized'], keep='first')
print(f"After title dedup: {len(df)}")

# 3. Abstract í•„ìˆ˜
df = df[df['abstract'].notna()]
print(f"With abstract: {len(df)}")

# ì €ì¥
df.to_csv("data/processed/deduplicated.csv", index=False)
```

#### Step 2.2: Stage 2 - Screening (ê´€ë ¨ì„± ì ìˆ˜í™”)

**AI failure_HR ë‹¤ì°¨ì› ìŠ¤ì½”ì–´ë§**:

| ì°¨ì› | ë°°ì  | ì„¤ëª… |
|------|------|------|
| HR Domain Score | 0-10 | HRM/HRD í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜ |
| AI Score | 0-5 | AI/ML í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜ |
| Risk Score | 0-5 | Bias/ethics í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜ |
| Context Score | 0-10 | ë§¥ë½ ê²€ì¦ì (employee, workplace) |
| Exclusion Penalty | -20 or 0 | ì œì™¸ í‚¤ì›Œë“œ 2ê°œ ì´ìƒ ì‹œ -20 |
| Title Bonus | +10 or 0 | ì œëª©ì— ë„ë©”ì¸ í‚¤ì›Œë“œ ìˆìœ¼ë©´ +10 |

**ì´ì  ë²”ìœ„**: -20 ~ 50ì 
**í†µê³¼ ê¸°ì¤€**: â‰¥ 15ì  (AI failure_HR ì‚¬ë¡€)

**ì—°êµ¬ì ì»¤ìŠ¤í„°ë§ˆì´ì§• ì˜ˆì‹œ**:

```python
# prisma_screening.py
from backend.core.prisma_pipeline import LiteratureReviewPRISMA
from backend.core.research_profile import ResearchProfile

# ì—°êµ¬ í”„ë¡œíŒŒì¼ ë¡œë“œ
profile = ResearchProfile.from_yaml("config/research_profiles/ai_education_adoption.yaml")

# PRISMA íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
prisma = LiteratureReviewPRISMA(profile)

# ë¬¸ì„œ ë¡œë“œ
import pandas as pd
from langchain.schema import Document

df = pd.read_csv("data/processed/deduplicated.csv")

documents = []
for _, row in df.iterrows():
    doc = Document(
        page_content=row['abstract'],
        metadata={
            'title': row['title'],
            'doi': row['doi'],
            'year': row['year'],
            'authors': row['authors']
        }
    )
    documents.append(doc)

# Stage 1-4 ì‹¤í–‰
result = prisma.run_full_pipeline(documents)

print(f"Stage 1 (Identification): {result.stage1.unique_count} unique papers")
print(f"Stage 2 (Screening): {result.stage2.passed_count} passed")
print(f"Stage 3 (Eligibility): {result.stage3.passed_count} eligible")
print(f"Stage 4 (Included): {result.stage4.included_count} included")

# ê²°ê³¼ ì €ì¥
included_df = pd.DataFrame([
    {**doc.metadata, 'prisma_score': doc.metadata.get('prisma_score')}
    for doc in result.stage4.documents
])
included_df.to_csv("data/processed/prisma_included.csv", index=False)

# PRISMA í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
mermaid = prisma.generate_prisma_flow_mermaid(result)
with open("outputs/prisma_flow.md", "w") as f:
    f.write(mermaid)
```

**AI failure_HR ì„±ê³¼**:
- Stage 2 ì…ë ¥: 9,192í¸
- Stage 2 ì¶œë ¥: 592í¸ (6.4% í†µê³¼ìœ¨)
- ì œì™¸ ì´ìœ :
  - Low HR relevance: ~60%
  - Medical/clinical context: ~25%
  - Pure technical (no HR): ~15%

#### Step 2.3: Stage 3 - Eligibility (Full-text í‰ê°€)

**í˜„ì¬ êµ¬í˜„**: í‚¤ì›Œë“œ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê²€ì¦
**í–¥í›„ (v1.1)**: LLM ê¸°ë°˜ ì‹œë§¨í‹± ì»¨í…ìŠ¤íŠ¸ í‰ê°€

```python
# í˜„ì¬: ì»¨í…ìŠ¤íŠ¸ ê²€ì¦ì ì¹´ìš´íŠ¸
context_score = sum(
    1 for kw in profile.context_validators
    if kw.lower() in full_text.lower()
)
# 0-20ì  ë¶€ì—¬

# í–¥í›„ (LLM ê°•í™”):
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-5-sonnet-20241022")

prompt = f"""
Does this paper discuss {profile.name} in a meaningful context?

Title: {doc.metadata['title']}
Abstract: {doc.page_content}

Criteria:
- Domain relevance (0-10): {', '.join(profile.domain_keywords)}
- Context validators: {', '.join(profile.context_validators)}

Provide score 0-20 and brief justification.
"""

response = llm.invoke(prompt)
llm_score = parse_score(response.content)
```

#### Step 2.4: Stage 4 - Inclusion (ìµœì¢… ê²°ì •)

**ìë™ í¬í•¨ ê¸°ì¤€**:
- PRISMA ì ìˆ˜ â‰¥ min_inclusion_score (ì˜ˆ: 60ì )

**ìˆ˜ë™ ê²€í†  í**:
- ì ìˆ˜ review_threshold ~ min_inclusion_score (ì˜ˆ: 40-59ì )
- ì—°êµ¬ìê°€ ì§ì ‘ íŒë‹¨ í•„ìš”

**AI failure_HR ê²°ê³¼**:
- ìë™ í¬í•¨: 592í¸
- ìˆ˜ë™ ê²€í† : 0í¸ (thresholdë¥¼ ë‚®ê²Œ ì„¤ì •)

---

### Phase 3: PDF ë‹¤ìš´ë¡œë“œ & ì „ì²˜ë¦¬ (2-3ì¼)

#### Step 3.1: PDF ë‹¤ìš´ë¡œë“œ ì „ëµ

**AI failure_HR ì‚¬ë¡€**:
- ëŒ€ìƒ: 592í¸
- ë‹¤ìš´ë¡œë“œ ì„±ê³µ: 316í¸ (53.4%)
- ì‹¤íŒ¨ ì›ì¸:
  - Paywall: ~30%
  - ì˜ëª»ëœ URL: ~10%
  - ì„œë²„ ì˜¤ë¥˜: ~7%

**ë””ë ‰í„°ë¦¬ êµ¬ì¡°** (AI failure_HR ì‹¤ì œ):
```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ openalex_hr_ai_risk_20251004.csv
â”‚   â”œâ”€â”€ semantic_scholar_comprehensive_20251004.csv
â”‚   â””â”€â”€ arxiv_comprehensive_20251004.csv
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ deduplicated.csv
â”‚   â”œâ”€â”€ prisma_screened.csv
â”‚   â””â”€â”€ final_screened_papers.csv (592 papers)
â””â”€â”€ pdfs/
    â”œâ”€â”€ openalex/      (249 PDFs)
    â”œâ”€â”€ semantic_scholar/  (67 PDFs)
    â””â”€â”€ arxiv/         (0 PDFs - to download)
```

**ë‹¤ìš´ë¡œë“œ ì½”ë“œ**:
```python
# download_pdfs.py
import requests
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import time

df = pd.read_csv("data/processed/prisma_included.csv")

# ì†ŒìŠ¤ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì €ì¥
for source in ['openalex', 'semantic_scholar', 'arxiv']:
    source_df = df[df['source'] == source]
    output_dir = Path(f"data/pdfs/{source}")
    output_dir.mkdir(parents=True, exist_ok=True)

    success = 0
    failed = []

    for idx, row in tqdm(source_df.iterrows(), total=len(source_df), desc=f"Downloading {source}"):
        doi = row['doi']
        pdf_url = row['pdf_url']

        if pd.isna(pdf_url):
            failed.append((doi, "No URL"))
            continue

        try:
            # DOIë¥¼ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš© (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
            safe_doi = doi.replace('/', '_').replace(':', '_')
            output_path = output_dir / f"{safe_doi}.pdf"

            if output_path.exists():
                success += 1
                continue

            response = requests.get(pdf_url, timeout=30)
            response.raise_for_status()

            with open(output_path, 'wb') as f:
                f.write(response.content)

            success += 1
            time.sleep(1)  # Rate limiting

        except Exception as e:
            failed.append((doi, str(e)))

    print(f"{source}: {success} success, {len(failed)} failed")

# ì‹¤íŒ¨ ëª©ë¡ ì €ì¥
failed_df = pd.DataFrame(failed, columns=['doi', 'reason'])
failed_df.to_csv("data/processed/download_failed.csv", index=False)
```

#### Step 3.2: í…ìŠ¤íŠ¸ ì¶”ì¶œ

**AI failure_HR ì‚¬ìš© ë„êµ¬**:
1. **PyMuPDF** (Primary) - ë¹ ë¥´ê³  ì •í™•
2. **pdfplumber** (Backup) - í‘œ ì¶”ì¶œ ìš°ìˆ˜
3. **Tesseract OCR** (Fallback) - ìŠ¤ìº” PDFìš©

**ì¶”ì¶œ ì½”ë“œ**:
```python
# extract_text.py
import fitz  # PyMuPDF
from pathlib import Path
import json

pdf_dir = Path("data/pdfs")
output_dir = Path("data/texts")
output_dir.mkdir(exist_ok=True)

for pdf_file in pdf_dir.rglob("*.pdf"):
    try:
        doc = fitz.open(pdf_file)

        # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        full_text = ""
        for page in doc:
            full_text += page.get_text()

        # ì„¹ì…˜ë³„ ì¶”ì¶œ ì‹œë„ (íœ´ë¦¬ìŠ¤í‹±)
        sections = {
            'title': extract_title(doc[0]),
            'abstract': extract_section(full_text, "abstract"),
            'introduction': extract_section(full_text, "introduction"),
            'methods': extract_section(full_text, "method"),
            'results': extract_section(full_text, "results"),
            'discussion': extract_section(full_text, "discussion"),
            'conclusion': extract_section(full_text, "conclusion"),
            'references': extract_section(full_text, "references")
        }

        # ì €ì¥
        output_file = output_dir / f"{pdf_file.stem}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'full_text': full_text,
                'sections': sections,
                'metadata': {
                    'source_pdf': str(pdf_file),
                    'num_pages': len(doc),
                    'text_length': len(full_text)
                }
            }, f, indent=2, ensure_ascii=False)

        doc.close()

    except Exception as e:
        print(f"Failed {pdf_file.name}: {e}")
```

**AI failure_HR ì„±ê³¼**:
- ì¶”ì¶œ ì„±ê³µë¥ : 98% (310/316 PDFs)
- í‰ê·  í˜ì´ì§€: 12í˜ì´ì§€
- í‰ê·  ë‹¨ì–´ìˆ˜: 8,500 ë‹¨ì–´

---

### Phase 4: ë²¡í„°í™” & RAG êµ¬ì¶• (1ì¼)

#### Step 4.1: í”„ë¡œì íŠ¸ë³„ Collection ìƒì„±

**í•µì‹¬ ì›ì¹™**: í”„ë¡œì íŠ¸ë³„ë¡œ ë…ë¦½ëœ ë²¡í„° ì»¬ë ‰ì…˜ ìœ ì§€

**ë””ë ‰í„°ë¦¬ êµ¬ì¡°**:
```
data/
â””â”€â”€ vector_db/
    â”œâ”€â”€ ai_education_adoption/     # í”„ë¡œì íŠ¸ 1
    â”‚   â”œâ”€â”€ chroma.sqlite3
    â”‚   â””â”€â”€ index/
    â”œâ”€â”€ ehr_physician_workflow/    # í”„ë¡œì íŠ¸ 2
    â”‚   â”œâ”€â”€ chroma.sqlite3
    â”‚   â””â”€â”€ index/
    â””â”€â”€ minimum_wage_employment/   # í”„ë¡œì íŠ¸ 3
        â”œâ”€â”€ chroma.sqlite3
        â””â”€â”€ index/
```

**ë²¡í„°í™” ì½”ë“œ**:
```python
# create_vector_db.py
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
import json
from pathlib import Path
from tqdm import tqdm

# í”„ë¡œì íŠ¸ëª… ì§€ì •
PROJECT_NAME = "ai_education_adoption"

# ì„ë² ë”© ëª¨ë¸ (AI failure_HRì™€ ë™ì¼)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

# í…ìŠ¤íŠ¸ ë¶„í• ê¸°
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # AI failure_HR ê¸°ë³¸ê°’
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""]
)

# ë¬¸ì„œ ë¡œë“œ
texts_dir = Path("data/texts")
documents = []

for json_file in tqdm(list(texts_dir.glob("*.json")), desc="Loading texts"):
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # Abstract + Introduction + Conclusion ì‚¬ìš© (AI failure_HR ì „ëµ)
    text_parts = []
    if data['sections'].get('abstract'):
        text_parts.append(data['sections']['abstract'])
    if data['sections'].get('introduction'):
        text_parts.append(data['sections']['introduction'])
    if data['sections'].get('conclusion'):
        text_parts.append(data['sections']['conclusion'])

    combined_text = "\n\n".join(text_parts)

    # ì²­í¬ ë¶„í• 
    chunks = text_splitter.split_text(combined_text)

    for i, chunk in enumerate(chunks):
        doc = Document(
            page_content=chunk,
            metadata={
                'source': str(json_file),
                'chunk_id': i,
                'total_chunks': len(chunks),
                **data['metadata']
            }
        )
        documents.append(doc)

print(f"Total chunks: {len(documents)}")

# Chroma DB ìƒì„±
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory=f"data/vector_db/{PROJECT_NAME}",
    collection_name=PROJECT_NAME
)

print(f"Vector DB created at data/vector_db/{PROJECT_NAME}")
print(f"Collection: {PROJECT_NAME}, Documents: {vectorstore._collection.count()}")
```

**AI failure_HR ì˜ˆìƒ ê·œëª¨**:
- 592í¸ ë…¼ë¬¸
- í‰ê·  12í˜ì´ì§€
- ì²­í¬ í¬ê¸° 1000ì
- ì˜ˆìƒ ì´ ì²­í¬: ~35,000ê°œ
- ë²¡í„° DB í¬ê¸°: ~2-3GB

#### Step 4.2: RAG ì¿¼ë¦¬ ì¸í„°í˜ì´ìŠ¤

**ì—°êµ¬ì ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**:

```python
# query_rag.py
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_anthropic import ChatAnthropic

PROJECT_NAME = "ai_education_adoption"

# ë²¡í„° DB ë¡œë“œ
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma(
    persist_directory=f"data/vector_db/{PROJECT_NAME}",
    embedding_function=embeddings,
    collection_name=PROJECT_NAME
)

# LLM ì´ˆê¸°í™”
llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.3
)

def query_literature(question: str, top_k: int = 5):
    """
    ë¬¸í—Œ RAG ì§ˆì˜

    Args:
        question: ì—°êµ¬ ì§ˆë¬¸
        top_k: ê²€ìƒ‰í•  ë¬¸í—Œ ìˆ˜

    Returns:
        ë‹µë³€ + ì¸ìš© ë¬¸í—Œ
    """
    # 1. ê´€ë ¨ ë¬¸í—Œ ê²€ìƒ‰
    docs = vectorstore.similarity_search(question, k=top_k)

    # 2. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    context = "\n\n".join([
        f"[{i+1}] {doc.page_content}\nSource: {doc.metadata['source']}"
        for i, doc in enumerate(docs)
    ])

    # 3. LLMì— ì§ˆì˜
    prompt = f"""
You are a research assistant helping with literature review.

Question: {question}

Context from {top_k} relevant papers:
{context}

Provide a comprehensive answer based on the literature. Cite sources as [1], [2], etc.
"""

    response = llm.invoke(prompt)

    return {
        'answer': response.content,
        'sources': [
            {
                'id': i+1,
                'excerpt': doc.page_content[:200],
                'source': doc.metadata['source']
            }
            for i, doc in enumerate(docs)
        ]
    }

# ì‚¬ìš© ì˜ˆì‹œ
result = query_literature(
    "What are the main barriers teachers face when adopting AI in the classroom?"
)

print("Answer:", result['answer'])
print("\nSources:")
for source in result['sources']:
    print(f"[{source['id']}] {source['excerpt']}... ({source['source']})")
```

**ì—°êµ¬ì ì§ˆì˜ ì˜ˆì‹œ (ë¶„ì•¼ë³„)**:

**êµìœ¡í•™**:
```python
questions = [
    "What are teacher self-efficacy factors for AI adoption?",
    "How does school infrastructure affect AI implementation?",
    "What professional development is needed for AI integration?",
    "What are student outcomes from AI-assisted learning?",
    "Compare TAM and UTAUT models for EdTech adoption"
]
```

**ì˜í•™**:
```python
questions = [
    "What are the main EHR usability issues reported by physicians?",
    "How does EHR use affect clinical workflow efficiency?",
    "What training strategies improve EHR adoption rates?",
    "What is the relationship between EHR alerts and alert fatigue?",
    "Compare pre- and post-EHR patient satisfaction scores"
]
```

**ê²½ì œí•™**:
```python
questions = [
    "What is the employment elasticity of minimum wage increases?",
    "Compare employment effects in different industries?",
    "What are the long-term effects of minimum wage on wages?",
    "How do minimum wage effects differ by region?",
    "Synthesize evidence on disemployment effects"
]
```

---

### Phase 5: ê³ ê¸‰ ë¶„ì„ & ë…¼ë¬¸ ì‘ì„± (ì§„í–‰ ì¤‘)

#### Step 5.1: BERTopic í´ëŸ¬ìŠ¤í„°ë§

**AI failure_HR RQ1 ë‹µë³€ì„ ìœ„í•œ ë¶„ì„**:

```python
# bertopic_analysis.py
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import pandas as pd

# ë¬¸ì„œ ë¡œë“œ (ì „ì²´ í…ìŠ¤íŠ¸)
df = pd.read_csv("data/processed/final_screened_papers.csv")
documents = df['full_text'].tolist()

# ì„ë² ë”© ëª¨ë¸
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# BERTopic ì´ˆê¸°í™”
topic_model = BERTopic(
    language="english",
    calculate_probabilities=True,
    min_topic_size=15,  # ìµœì†Œ 15í¸/í† í”½
    nr_topics='auto',
    n_gram_range=(1, 3),
    verbose=True
)

# ì„ë² ë”© ìƒì„±
embeddings = embedding_model.encode(documents, show_progress_bar=True)

# í† í”½ ëª¨ë¸ë§
topics, probs = topic_model.fit_transform(documents, embeddings)

# í† í”½ ì •ë³´ ì €ì¥
topic_info = topic_model.get_topic_info()
topic_info.to_csv("outputs/topic_info.csv", index=False)

# í† í”½ ì‹œê°í™”
fig = topic_model.visualize_topics()
fig.write_html("outputs/topic_visualization.html")

# í† í”½ë³„ ëŒ€í‘œ ë¬¸ì„œ
for topic_id in range(len(topic_info)-1):  # -1ì€ outlier topic
    print(f"\nTopic {topic_id}:")
    print(topic_model.get_topic(topic_id)[:10])  # Top 10 keywords
    print(topic_model.get_representative_docs(topic_id)[:3])  # Top 3 papers
```

**AI failure_HR ì˜ˆìƒ í† í”½ (10-20ê°œ)**:
1. Hiring Bias & Algorithmic Fairness
2. Performance Evaluation & Employee Monitoring
3. People Analytics & Privacy Concerns
4. Predictive HR & Turnover Prediction
5. Resume Screening & Selection Bias
6. Learning Analytics in Corporate Training
7. AI-powered Skill Gap Assessment
8. Algorithmic Management & Worker Rights
9. Explainability & Transparency in HR AI
10. Regulatory Compliance & GDPR

#### Step 5.2: ì‹œê°„ì  ì§„í™” ë¶„ì„

**AI failure_HR RQ2 ë‹µë³€**:

```python
# temporal_evolution.py
from bertopic import BERTopic
import pandas as pd

# í† í”½ ëª¨ë¸ ë¡œë“œ
topic_model = BERTopic.load("outputs/topic_model")

# ì—°ë„ë³„ í† í”½ ë¶„í¬
df['year'] = pd.to_datetime(df['publication_date']).dt.year

topics_over_time = topic_model.topics_over_time(
    documents,
    df['year'],
    nr_bins=10  # 2015-2024ë¥¼ 10ê°œ êµ¬ê°„ìœ¼ë¡œ
)

# ì‹œê°í™”
fig = topic_model.visualize_topics_over_time(topics_over_time)
fig.write_html("outputs/topics_over_time.html")

# ì„±ì¥ë¥  ê³„ì‚°
growth_rates = []
for topic_id in range(len(topic_info)-1):
    topic_papers = df[df['topic'] == topic_id]
    early = topic_papers[topic_papers['year'] <= 2019]
    recent = topic_papers[topic_papers['year'] >= 2022]
    growth_rate = (len(recent) - len(early)) / len(early) * 100 if len(early) > 0 else 0
    growth_rates.append({
        'topic_id': topic_id,
        'topic_name': get_topic_name(topic_id),
        'early_count': len(early),
        'recent_count': len(recent),
        'growth_rate': growth_rate
    })

growth_df = pd.DataFrame(growth_rates).sort_values('growth_rate', ascending=False)
growth_df.to_csv("outputs/topic_growth_rates.csv", index=False)
```

#### Step 5.3: HRM vs HRD ë¹„êµ

**AI failure_HR RQ3 ë‹µë³€**:

```python
# compare_hrm_hrd.py
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("data/processed/final_screened_papers.csv")

# HRM vs HRD ë¶„ë¥˜ (PRISMA ì‹œ ì´ë¯¸ ë¶„ë¥˜ë¨)
hrm_papers = df[df['hr_category'] == 'HRM']
hrd_papers = df[df['hr_category'] == 'HRD']

print(f"HRM papers: {len(hrm_papers)} ({len(hrm_papers)/len(df)*100:.1f}%)")
print(f"HRD papers: {len(hrd_papers)} ({len(hrd_papers)/len(df)*100:.1f}%)")

# í† í”½ ë¶„í¬ ë¹„êµ
hrm_topics = hrm_papers['topic'].value_counts()
hrd_topics = hrd_papers['topic'].value_counts()

# ì‹œê°„ì  ì„±ìˆ™ë„
hrm_first_year = hrm_papers.groupby('topic')['year'].min()
hrd_first_year = hrd_papers.groupby('topic')['year'].min()

maturity_comparison = pd.DataFrame({
    'hrm_first_year': hrm_first_year,
    'hrd_first_year': hrd_first_year,
    'maturity_gap': hrd_first_year - hrm_first_year
}).sort_values('maturity_gap', ascending=False)

print("\nTopic Maturity Comparison (HRD lag behind HRM):")
print(maturity_comparison.head(10))

# ì¸ìš© ì˜í–¥ë ¥ ë¹„êµ
hrm_citations = hrm_papers.groupby('topic')['cited_by_count'].mean()
hrd_citations = hrd_papers.groupby('topic')['cited_by_count'].mean()

citation_comparison = pd.DataFrame({
    'hrm_avg_citations': hrm_citations,
    'hrd_avg_citations': hrd_citations,
    'citation_gap': hrm_citations - hrd_citations
})

# ì‹œê°í™”
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Topic size comparison
axes[0,0].bar(['HRM', 'HRD'], [len(hrm_papers), len(hrd_papers)])
axes[0,0].set_title('Research Volume: HRM vs HRD')
axes[0,0].set_ylabel('Number of Papers')

# 2. Temporal evolution
hrm_yearly = hrm_papers.groupby('year').size()
hrd_yearly = hrd_papers.groupby('year').size()
axes[0,1].plot(hrm_yearly.index, hrm_yearly.values, label='HRM', marker='o')
axes[0,1].plot(hrd_yearly.index, hrd_yearly.values, label='HRD', marker='s')
axes[0,1].set_title('Temporal Evolution: HRM vs HRD')
axes[0,1].legend()

# 3. Citation impact
axes[1,0].scatter(hrm_citations.values, hrd_citations.values)
axes[1,0].plot([0, max(hrm_citations.max(), hrd_citations.max())],
               [0, max(hrm_citations.max(), hrd_citations.max())], 'r--')
axes[1,0].set_xlabel('HRM Avg Citations')
axes[1,0].set_ylabel('HRD Avg Citations')
axes[1,0].set_title('Citation Impact Comparison')

# 4. Maturity gap
maturity_comparison['maturity_gap'].plot(kind='barh', ax=axes[1,1])
axes[1,1].set_title('Topic Maturity Gap (years HRD lags HRM)')

plt.tight_layout()
plt.savefig("outputs/hrm_vs_hrd_comparison.png", dpi=300)
```

**AI failure_HR ì˜ˆìƒ ê²°ê³¼**:
- HRM ë…¼ë¬¸: 471í¸ (79.6%)
- HRD ë…¼ë¬¸: 105í¸ (17.7%)
- HRM í‰ê·  ì¶œí˜„ ì—°ë„: 2019
- HRD í‰ê·  ì¶œí˜„ ì—°ë„: 2021
- **ì„±ìˆ™ë„ ê²©ì°¨**: 2ë…„ (HRDê°€ HRMë³´ë‹¤ ë’¤ì²˜ì§)
- **í•´ì„**: ê³ ìš© ì°¨ë³„ ê·œì œ(EEOC)ê°€ HRM ì—°êµ¬ë¥¼ ì´‰ì§„, HRDëŠ” ê·œì œ ì••ë ¥ ë‚®ìŒ

---

## ğŸ“Š ResearcherRAG ì‹¤ì „ í™œìš© ì‚¬ë¡€ (í•™ë¬¸ ë¶„ì•¼ë³„)

### ì‚¬ë¡€ 1: êµìœ¡í•™ (AI êµìœ¡ ì±„íƒ ì—°êµ¬)

**ì—°êµ¬ì**: ë°•OO êµìˆ˜ (êµìœ¡ê³µí•™)

**ì—°êµ¬ ì§ˆë¬¸**: "K-12 êµì‚¬ì˜ AI êµìœ¡ ë„êµ¬ ì±„íƒì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ì€?"

**ì›Œí¬í”Œë¡œìš°**:
1. **ë°ì´í„° ìˆ˜ì§‘** (OpenAlex + ERIC)
   - ê²€ìƒ‰ì–´: AI adoption, teacher, K-12, EdTech
   - ìˆ˜ì§‘: 1,247í¸
2. **PRISMA ìŠ¤í¬ë¦¬ë‹**
   - Profile: `k12_ai_adoption.yaml`
   - í¬í•¨: 287í¸ (23% ì„ íƒë¥ )
3. **PDF ë‹¤ìš´ë¡œë“œ**: 189í¸ (65.8%)
4. **ë²¡í„°í™”**: ChromaDB `k12_ai_adoption` collection
5. **RAG í™œìš©**:
   - "What are self-efficacy factors?" â†’ ê´€ë ¨ 15í¸ ì¢…í•©
   - "Compare TAM and TPB models" â†’ ì´ë¡  ë¹„êµ
   - "School infrastructure barriers?" â†’ ì¥ë²½ ëª©ë¡í™”

**ì„±ê³¼**:
- ë¬¸í—Œê³ ì°° ì‹œê°„: 6ì£¼ â†’ 2ì£¼ (67% ë‹¨ì¶•)
- ë…¼ë¬¸ ì‘ì„±: Abstract ì´ˆì•ˆ 1ì¼ ë§Œì— ì™„ì„±
- ì¸ìš© ì •í™•ë„: 100% (RAGê°€ ì •í™•í•œ ì¶œì²˜ ì œê³µ)

---

### ì‚¬ë¡€ 2: ì˜í•™ (EHR ë„ì… ì˜í–¥ ì—°êµ¬)

**ì—°êµ¬ì**: ê¹€OO êµìˆ˜ (ì˜ë£Œì •ë³´í•™)

**ì—°êµ¬ ì§ˆë¬¸**: "ì „ìê±´ê°•ê¸°ë¡(EHR) ë„ì…ì´ ì˜ì‚¬ ì—…ë¬´ íš¨ìœ¨ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥"

**ì›Œí¬í”Œë¡œìš°**:
1. **ë°ì´í„° ìˆ˜ì§‘** (PubMed + OpenAlex)
   - MeSH terms: Electronic Health Records, Physician Workflow
   - ìˆ˜ì§‘: 3,421í¸
2. **PRISMA ìŠ¤í¬ë¦¬ë‹**
   - Profile: `ehr_physician_workflow.yaml`
   - Exclusion: nursing, patient perspective
   - í¬í•¨: 412í¸ (12% ì„ íƒë¥ )
3. **PDF ë‹¤ìš´ë¡œë“œ**: 378í¸ (91.7%, PMC open access)
4. **ë²¡í„°í™”**: ChromaDB `ehr_physician` collection
5. **ë©”íƒ€ë¶„ì„ ì¤€ë¹„**:
   ```python
   # íš¨ê³¼í¬ê¸° ì¶”ì¶œ
   query = "Extract effect sizes (Cohen's d, OR, HR) for EHR impact on efficiency"
   results = query_literature(query, top_k=50)
   # LLMì´ í‘œì¤€í™”ëœ íš¨ê³¼í¬ê¸° ì¶”ì¶œ
   ```

**ì„±ê³¼**:
- ë©”íƒ€ë¶„ì„ ì¤€ë¹„ ì‹œê°„: 4ì£¼ â†’ 1ì£¼
- íš¨ê³¼í¬ê¸° ì¶”ì¶œ: ìˆ˜ë™ 78ê°œ â†’ RAG ë³´ì¡° 156ê°œ
- ì´ì§ˆì„± ë¶„ì„: ì¡°ì ˆë³€ìˆ˜ ìë™ ì‹ë³„ (EHR ìœ í˜•, ì§„ë£Œê³¼)

---

### ì‚¬ë¡€ 3: ê²½ì œí•™ (ìµœì €ì„ê¸ˆ ê³ ìš© íš¨ê³¼)

**ì—°êµ¬ì**: ì´OO êµìˆ˜ (ë…¸ë™ê²½ì œí•™)

**ì—°êµ¬ ì§ˆë¬¸**: "ìµœì €ì„ê¸ˆ ì¸ìƒì´ ê³ ìš©ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼: ë©”íƒ€íšŒê·€ë¶„ì„"

**ì›Œí¬í”Œë¡œìš°**:
1. **ë°ì´í„° ìˆ˜ì§‘** (EconPapers + OpenAlex)
   - ê²€ìƒ‰ì–´: minimum wage, employment effect, elasticity
   - ìˆ˜ì§‘: 892í¸
2. **PRISMA ìŠ¤í¬ë¦¬ë‹**
   - Profile: `minimum_wage_employment.yaml`
   - Exclusion: developing countries (OECDë§Œ)
   - í¬í•¨: 127í¸ (14% ì„ íƒë¥ )
3. **PDF ë‹¤ìš´ë¡œë“œ**: 94í¸ (74%, working papers í¬í•¨)
4. **ë©”íƒ€ë°ì´í„° ì¶”ì¶œ**:
   ```python
   query = """
   Extract from each paper:
   - Employment elasticity estimate
   - Standard error
   - Country
   - Time period
   - Industry
   - Estimation method (DID, RD, IV)
   """
   metadata_df = extract_meta_analysis_data(query)
   ```

**ì„±ê³¼**:
- ë©”íƒ€ë¶„ì„ ë°ì´í„°ì…‹ êµ¬ì¶•: 8ì£¼ â†’ 2ì£¼
- ë…¼ë¬¸ë‹¹ í‰ê·  ì¶”ì¶œ ì‹œê°„: 45ë¶„ â†’ 3ë¶„ (LLM ë³´ì¡°)
- í¬í•¨ëœ ì¶”ì •ì¹˜: 342ê°œ (ê¸°ì¡´ ë©”íƒ€ë¶„ì„ ëŒ€ë¹„ 2ë°°)

---

### ì‚¬ë¡€ 4: ì‹¬ë¦¬í•™ (ì›ê²© ì‹¬ë¦¬ì¹˜ë£Œ íš¨ê³¼ì„±)

**ì—°êµ¬ì**: ìµœOO êµìˆ˜ (ì„ìƒì‹¬ë¦¬í•™)

**ì—°êµ¬ ì§ˆë¬¸**: "ì›ê²© ì‹¬ë¦¬ì¹˜ë£Œì˜ íš¨ê³¼ì„±ì€ ëŒ€ë©´ ì¹˜ë£Œì™€ ì°¨ì´ê°€ ìˆëŠ”ê°€?"

**ì›Œí¬í”Œë¡œìš°**:
1. **ë°ì´í„° ìˆ˜ì§‘** (PsycINFO + OpenAlex)
   - ê²€ìƒ‰ì–´: teletherapy, online therapy, psychotherapy, RCT
   - ìˆ˜ì§‘: 567í¸
2. **PRISMA ìŠ¤í¬ë¦¬ë‹**
   - Profile: `teletherapy_effectiveness.yaml`
   - Inclusion: RCT, control group í•„ìˆ˜
   - í¬í•¨: 78í¸ (13.7% ì„ íƒë¥ )
3. **í’ˆì§ˆ í‰ê°€ ìë™í™”**:
   ```python
   query = """
   Assess study quality using Cochrane RoB 2 tool:
   - Randomization
   - Allocation concealment
   - Blinding
   - Attrition
   - Selective reporting
   """
   quality_scores = extract_quality_assessment(query)
   ```

**ì„±ê³¼**:
- Cochrane ë¦¬ë·° ìˆ˜ì¤€ í’ˆì§ˆ í‰ê°€: 2ì£¼ â†’ 3ì¼
- íš¨ê³¼í¬ê¸° í‘œì¤€í™”: Hedge's g ìë™ ê³„ì‚°
- ë¯¼ê°ë„ ë¶„ì„: ê³ í’ˆì§ˆ RCT only ìë™ í•„í„°

---

## ğŸ“ í•™ë¬¸ ë¶„ì•¼ë³„ Research Profile í…œí”Œë¦¿

### êµìœ¡í•™ (AI ì±„íƒ)

```yaml
name: "AI Adoption in Education"
domain_keywords:
  - education, teacher, student, school, classroom
  - pedagogy, instruction, curriculum, learning
method_keywords:
  - AI, EdTech, digital tool, technology integration
topic_keywords:
  - adoption, acceptance, barrier, facilitator, readiness
  - TAM, UTAUT, self-efficacy, attitude
exclusion_keywords:
  - medical education, clinical training, nursing
context_validators:
  - teacher, student, classroom, school
thresholds:
  screening: 30
  eligibility: 50
```

### ì˜í•™ (EHR ì˜í–¥)

```yaml
name: "EHR Impact on Physician Workflow"
domain_keywords:
  - physician, clinician, doctor, healthcare provider
  - hospital, clinic, primary care, emergency department
method_keywords:
  - EHR, EMR, electronic health record, CPOE
topic_keywords:
  - workflow, efficiency, burden, satisfaction, usability
  - burnout, alert fatigue, documentation time
exclusion_keywords:
  - nursing, patient perspective, patient portal
context_validators:
  - physician, clinician, clinical workflow
thresholds:
  screening: 35
  eligibility: 55
```

### ê²½ì œí•™ (ìµœì €ì„ê¸ˆ)

```yaml
name: "Minimum Wage Employment Effects"
domain_keywords:
  - labor market, employment, unemployment, job
  - wage, earnings, income, compensation
method_keywords:
  - minimum wage, wage floor, wage policy
  - difference-in-differences, DID, RD, regression discontinuity
topic_keywords:
  - employment effect, job loss, elasticity
  - disemployment, labor demand, substitution
exclusion_keywords:
  - developing country, informal sector (if OECD focus)
context_validators:
  - worker, employee, labor force, employment
thresholds:
  screening: 30
  eligibility: 50
```

### ì‹¬ë¦¬í•™ (ì›ê²© ì¹˜ë£Œ)

```yaml
name: "Teletherapy Effectiveness"
domain_keywords:
  - psychotherapy, counseling, mental health treatment
  - therapy, intervention, clinical psychology
method_keywords:
  - teletherapy, online therapy, telehealth
  - videoconference, internet-based, digital
topic_keywords:
  - effectiveness, efficacy, outcome, symptom
  - RCT, randomized controlled trial, control group
exclusion_keywords:
  - pharmacotherapy, medication, drug treatment
context_validators:
  - patient, client, therapist, session
thresholds:
  screening: 35
  eligibility: 55
```

---

## ğŸš€ ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ì—°êµ¬ììš©)

### Week 1: ì¤€ë¹„ & ìˆ˜ì§‘

- [ ] ì—°êµ¬ ì§ˆë¬¸ ëª…í™•í™” (RQ 1-3ê°œ)
- [ ] Research Profile YAML ì‘ì„±
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ì„ ì • (OpenAlex, PubMed, etc.)
- [ ] ê²€ìƒ‰ ì¿¼ë¦¬ ì‘ì„± (5-10ê°œ)
- [ ] ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ (ëª©í‘œ: 500-2000í¸)
- [ ] CSV ì €ì¥ ë° ë°±ì—…

### Week 2: ìŠ¤í¬ë¦¬ë‹

- [ ] PRISMA Stage 1 (ì¤‘ë³µ ì œê±°)
- [ ] PRISMA Stage 2 (ê´€ë ¨ì„± ì ìˆ˜í™”)
- [ ] ì„ê³„ê°’ ì¡°ì • (pilot 10í¸ìœ¼ë¡œ ê²€ì¦)
- [ ] ìµœì¢… í¬í•¨ ë…¼ë¬¸ í™•ì •
- [ ] PRISMA í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
- [ ] prisma_included.csv ì €ì¥

### Week 3: PDF & ì „ì²˜ë¦¬

- [ ] PDF ë‹¤ìš´ë¡œë“œ (ëª©í‘œ: 70%+)
- [ ] í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyMuPDF)
- [ ] ì„¹ì…˜ë³„ íŒŒì‹± (Abstract, Intro, Methods, etc.)
- [ ] JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
- [ ] í’ˆì§ˆ ê²€ì‚¬ (ì¶”ì¶œ ì„±ê³µë¥  í™•ì¸)

### Week 4: ë²¡í„°í™” & RAG

- [ ] í”„ë¡œì íŠ¸ë³„ Vector DB ìƒì„±
- [ ] ì„ë² ë”© ìƒì„± (all-MiniLM-L6-v2)
- [ ] Chroma collection ì €ì¥
- [ ] RAG ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ (5-10ê°œ ì§ˆë¬¸)
- [ ] ë‹µë³€ í’ˆì§ˆ ê²€ì¦
- [ ] ì¸ìš© ì •í™•ë„ í™•ì¸

### Ongoing: ë¶„ì„ & ë…¼ë¬¸ ì‘ì„±

- [ ] BERTopic í´ëŸ¬ìŠ¤í„°ë§ (ì„ íƒ)
- [ ] ì‹œê°„ì  ì§„í™” ë¶„ì„ (ì„ íƒ)
- [ ] RAG ê¸°ë°˜ ë¬¸í—Œê³ ì°° ì‘ì„±
- [ ] ë©”íƒ€ë¶„ì„ ë°ì´í„° ì¶”ì¶œ (í•„ìš”ì‹œ)
- [ ] ì´ˆì•ˆ ì™„ì„± (Abstract, Intro, Methods)
- [ ] ë™ë£Œ ê²€í†  ë° ìˆ˜ì •

---

## ğŸ’¡ ì—°êµ¬ì FAQ

### Q1: PRISMA ì„ê³„ê°’ì„ ì–´ë–»ê²Œ ì •í• ê¹Œìš”?

**A**: Pilot ìŠ¤í¬ë¦¬ë‹ìœ¼ë¡œ ì¡°ì •

```python
# 10-20í¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
pilot_df = df.sample(20)

for threshold in [15, 20, 25, 30]:
    included = pilot_df[pilot_df['prisma_score'] >= threshold]
    print(f"Threshold {threshold}: {len(included)} papers")
    print(included[['title', 'prisma_score']].head())

# ì—°êµ¬ìê°€ ìˆ˜ë™ìœ¼ë¡œ 10í¸ í‰ê°€
# â†’ ìµœì  ì„ê³„ê°’ ê²°ì • (ì˜ˆ: 25ì )
```

**ê²½í—˜ ë²•ì¹™**:
- ë„ˆë¬´ ë‚®ìœ¼ë©´ (< 15): ê´€ë ¨ ì—†ëŠ” ë…¼ë¬¸ ë‹¤ìˆ˜ í¬í•¨
- ì ì ˆ (20-30): 90% ê´€ë ¨ì„±
- ë„ˆë¬´ ë†’ìœ¼ë©´ (> 35): ê´€ë ¨ ë…¼ë¬¸ ë†“ì¹  ìˆ˜ ìˆìŒ

### Q2: PDF ë‹¤ìš´ë¡œë“œê°€ ì•ˆ ë˜ëŠ” ê²½ìš°?

**A**: 3ë‹¨ê³„ ì „ëµ

1. **ê¸°ê´€ ì ‘ì†**: VPNìœ¼ë¡œ ëŒ€í•™ ë„ì„œê´€ ì ‘ì†
2. **ì €ì ìš”ì²­**: ResearchGate, ì €ì ì´ë©”ì¼ ì§ì ‘ ì—°ë½
3. **ëŒ€ì²´ ë²„ì „**: Preprint (arXiv, bioRxiv), Working paper

```python
# ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ë…¼ë¬¸ ì¶”ì¶œ
failed_df = pd.read_csv("data/processed/download_failed.csv")

# ê¸°ê´€ ê³„ì •ìœ¼ë¡œ ì¬ì‹œë„
for doi in failed_df['doi']:
    # ëŒ€í•™ í”„ë¡ì‹œ í†µí•´ ì¬ì‹œë„
    download_via_institutional_access(doi)
```

### Q3: ë²¡í„° DBê°€ ë„ˆë¬´ í¬ë©´?

**A**: ì„ íƒì  ì„ë² ë”©

**ì „ëµ 1**: Abstract + Conclusionë§Œ (ìš©ëŸ‰ 50% ê°ì†Œ)
```python
text_parts = [
    data['sections']['abstract'],
    data['sections']['conclusion']
]
```

**ì „ëµ 2**: ì••ì¶• ì„ë² ë”© ëª¨ë¸
```python
# all-MiniLM-L6-v2 (384 dim) â†’ all-MiniLM-L6-v2 (256 dim)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2-384"
)
```

**ì „ëµ 3**: í´ë¼ìš°ë“œ ë²¡í„° DB (Qdrant Cloud)
- ë¡œì»¬ í•œê³„: ~50,000 chunks
- í´ë¼ìš°ë“œ: ~1,000,000 chunks

### Q4: RAG ë‹µë³€ í’ˆì§ˆì´ ë‚®ìœ¼ë©´?

**A**: 3ê°€ì§€ ê°œì„  ë°©ë²•

**1. Top-K ì¦ê°€**
```python
# Before: top_k=5
docs = vectorstore.similarity_search(question, k=5)

# After: top_k=10 (ë” ë§ì€ ë§¥ë½)
docs = vectorstore.similarity_search(question, k=10)
```

**2. ì¿¼ë¦¬ ë¶„í•´** (LangGraph)
```python
# Before: ì§ì ‘ ì§ˆì˜
query = "What are teacher barriers to AI adoption?"

# After: í•˜ìœ„ ì§ˆë¬¸ ìƒì„±
sub_queries = [
    "What technical barriers do teachers face?",
    "What pedagogical barriers exist?",
    "What institutional barriers are there?"
]
# ê° í•˜ìœ„ ì§ˆë¬¸ ë‹µë³€ í›„ ì¢…í•©
```

**3. Re-ranking**
```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# LLMìœ¼ë¡œ ì¬ìˆœìœ„í™”
compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)
```

### Q5: í”„ë¡œì íŠ¸ê°€ ì—¬ëŸ¬ ê°œë©´?

**A**: í”„ë¡œì íŠ¸ë³„ ë…ë¦½ ê´€ë¦¬

**ë””ë ‰í„°ë¦¬ êµ¬ì¡°**:
```
/Volumes/External SSD/Projects/Research/
â”œâ”€â”€ Project1_AI_Education/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ config/research_profiles/ai_education.yaml
â”‚   â””â”€â”€ vector_db/
â”œâ”€â”€ Project2_EHR_Physician/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ config/research_profiles/ehr_physician.yaml
â”‚   â””â”€â”€ vector_db/
â””â”€â”€ Project3_MinWage_Employment/
    â”œâ”€â”€ data/
    â”œâ”€â”€ config/research_profiles/min_wage.yaml
    â””â”€â”€ vector_db/
```

**ì „í™˜ ë°©ë²•**:
```python
# í”„ë¡œì íŠ¸ ì „í™˜ì€ ë””ë ‰í„°ë¦¬ë§Œ ë³€ê²½
PROJECT_DIR = "/Volumes/External SSD/Projects/Research/Project1_AI_Education"

profile = ResearchProfile.from_yaml(f"{PROJECT_DIR}/config/research_profiles/ai_education.yaml")
vectorstore = Chroma(persist_directory=f"{PROJECT_DIR}/vector_db")
```

---

## ğŸ ë§ˆë¬´ë¦¬: ì—°êµ¬ì í–‰ë™ ì§€ì¹¨

### âœ… Do's (ê¶Œì¥)

1. **ëª…í™•í•œ ì—°êµ¬ ì§ˆë¬¸**: PRISMAëŠ” focused questionì— ìµœì í™”
2. **Pilot ìŠ¤í¬ë¦¬ë‹**: 10-20í¸ìœ¼ë¡œ ì„ê³„ê°’ ì¡°ì •
3. **í”„ë¡œì íŠ¸ë³„ ë¶„ë¦¬**: Vector DBë¥¼ í”„ë¡œì íŠ¸ë§ˆë‹¤ ë…ë¦½ ê´€ë¦¬
4. **ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©**: Abstractë§Œìœ¼ë¡  ë¶€ì¡±, Full-text í•„ìˆ˜
5. **ì¸ìš© ê²€ì¦**: RAG ë‹µë³€ì˜ ì¶œì²˜ë¥¼ í•­ìƒ ì›ë¬¸ í™•ì¸
6. **ë²„ì „ ê´€ë¦¬**: PRISMA ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ CSV ë°±ì—…
7. **ë¬¸ì„œí™”**: Research Profile YAMLì„ ë…¼ë¬¸ Methodsì— í¬í•¨

### âŒ Don'ts (ì§€ì–‘)

1. **ë„ˆë¬´ ë„“ì€ ì§ˆë¬¸**: "AI in society" â†’ ìˆ˜ë§Œ í¸, ìŠ¤í¬ë¦¬ë‹ ë¶ˆê°€
2. **ì„ê³„ê°’ ì¶”ì¸¡**: Pilot ì—†ì´ ì„ì˜ë¡œ ì„¤ì • â†’ í’ˆì§ˆ ì €í•˜
3. **PDF ìƒëµ**: Abstractë§Œìœ¼ë¡œ RAG â†’ ë‹µë³€ í”¼ìƒì 
4. **í”„ë¡œì íŠ¸ í˜¼ìš©**: ë²¡í„° DBì— ì—¬ëŸ¬ í”„ë¡œì íŠ¸ ì„ìœ¼ë©´ í˜¼ë€
5. **RAG ë§¹ì‹ **: ì¶œì²˜ í™•ì¸ ì—†ì´ ê·¸ëŒ€ë¡œ ì¸ìš© â†’ ì˜¤ë¥˜ ìœ„í—˜
6. **ë‹¨ë°œì„± ì‚¬ìš©**: RAGëŠ” iterative - ì§ˆë¬¸ ì •ì œí•˜ë©° ì‚¬ìš©
7. **Paywall í¬ê¸°**: 70% ë‹¤ìš´ë¡œë“œ ëª»í•˜ë©´ bias ë°œìƒ

---

## ğŸ“ ì§€ì› & ì»¤ë®¤ë‹ˆí‹°

**GitHub Issues**:
- ResearcherRAG ê¸°ìˆ  ë¬¸ì œ: https://github.com/HosungYou/researcherRAG/issues
- AI failure_HR ë°©ë²•ë¡  ë¬¸ì˜: hosung.you@example.com

**ì—°êµ¬ í˜‘ì—…**:
- PRISMA í”„ë¡œí† ì½œ ê³µìœ : ë™ì¼ ë¶„ì•¼ ì—°êµ¬ìë¼ë¦¬ Research Profile êµí™˜
- BERTopic ëª¨ë¸ ê³µìœ : ì‚¬ì „ í•™ìŠµëœ í† í”½ ëª¨ë¸ ì œê³µ ê°€ëŠ¥

**Citation**:
```bibtex
@software{researcherrag2025,
  title = {ResearcherRAG: PRISMA-Enhanced Literature Review System},
  author = {You, Hosung},
  year = {2025},
  url = {https://github.com/HosungYou/researcherRAG},
  version = {1.0.0}
}
```

---

**ì‘ì„±ì¼**: 2025-01-10
**ê¸°ë°˜ í”„ë¡œì íŠ¸**: AI failure_HR (592 papers, PRISMA screening)
**ëŒ€ìƒ**: ì‚¬íšŒê³¼í•™Â·ì˜í•™Â·ê²½ì œí•™ ë“± ì²´ê³„ì  ë¬¸í—Œê³ ì°° ì—°êµ¬ì
**ë²„ì „**: 1.0 (Practical Guide)

---

<div align="center">

### ğŸ“ "ResearcherRAGë¡œ ì—°êµ¬ì˜ ìƒˆ ì§€í‰ì„ ì—¬ì„¸ìš”"

**ì—°êµ¬ ì§ˆë¬¸ â†’ PRISMA â†’ RAG â†’ ë…¼ë¬¸ ì™„ì„±**

</div>
